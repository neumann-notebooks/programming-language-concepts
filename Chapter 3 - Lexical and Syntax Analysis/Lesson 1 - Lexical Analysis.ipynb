{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - Lexical Analysis\n",
    "Let's take a look into the process of lexical analysis, which is the first step in reading a source language file. In this lesson, we will be creating a fully funcioning scanner/lexer that will read in Wowza programming files (.wow).\n",
    "\n",
    "But, you may ask, \"what does the scanner actually do?\"\n",
    "\n",
    "Let's say we write a wowza program:\n",
    "```\n",
    "for i from 1 to 10 {\n",
    "    // Please do not lexically analyze me!!!\n",
    "    print('Count: ' + i);\n",
    "}\n",
    "```\n",
    "\n",
    "Simple. Now, our scanner should go through the scanner an apply the lexical rules we defined in the last chapter. If it finds the string \"print\", then it needs to return the string (the lexeme) and the type (the token). So, the goal of the scanner is to be able to get a sequence of lexeme/token pairs, like this:\n",
    "```javascript\n",
    "{lexeme: \"for\", token: \"keyword\"}\n",
    "{lexeme: \"i\", token: \"id\"}\n",
    "{lexeme: \"from\", token: \"keyword\"}\n",
    "{lexeme: \"1\", token: \"num_lit\"}\n",
    "{lexeme: \"to\", token: \"keyword\"}\n",
    "{lexeme: \"10\", token: \"num_lit\"}\n",
    "{lexeme: \"{\", token: \"begin_block\"}\n",
    "{lexeme: \"print\", token: \"keyword\"}\n",
    "{lexeme: \"(\", token: \"begin_paren\"}\n",
    "{lexeme: \"'Count: '\", token: \"string_lit\"}\n",
    "{lexeme: \"+\", token: \"add_op\"}\n",
    "{lexeme: \"i\", token: \"id\"}\n",
    "{lexeme: \")\", token: \"end_paren\"}\n",
    "{lexeme: \";\", token: \"end_stmt\"}\n",
    "{lexeme: \"}\", token: \"end_block\"}\n",
    "```\n",
    "\n",
    "Output looks a little ugly, but with a second glance you can see that this is much easier to deal with than just a list of characters.\n",
    "\n",
    "How can we put this in code? Well, let's see if we can define a new class in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scanner:\n",
    "    def __init__(self, path):\n",
    "        pass\n",
    "    def lex(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Scanner` is mainly a wrapper class around a single function called `lex()` which will work through the logic to return the next lexeme/token sequence in the message.\n",
    "\n",
    "Many lexers (scanners) are implemented using a Finite State Machine (FSM). This is simply an abstract model that helps to define relationships between different \"states\" and to keep track of a global state for the entire FSM. We can use this for our scanner by using each state in the machine as the token for the current lexeme we are looking at. \n",
    "\n",
    "For example, suppose the first character we see is a `<`. Can we assume that the token is `lt_op`? Not really. If the next character is `=`, then the token is `lte_op`. It all depends on the following characters. However, when we come across a `<` character, there is no way the resulting token can be a `add_op`. We need a way to keep track of the next possible tokens we may need to switch to when new characters come in. This is where the FSM comes in handy. With it we can define the logical steps/choices that we need to make in order to determine the correct token.\n",
    "\n",
    "There are three main components of a FSM: transitions, states, and the FSM itself. First, let's implement a `Transition` class which will hold a `target` State that we should go to if the `condition` is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(object):\n",
    "    \"\"\"Transition docstring.\"\"\"\n",
    "    def __init__(self, target, condition):\n",
    "        self.target = target\n",
    "        self.condition = condition\n",
    "    def get_state(self):\n",
    "        return self.target\n",
    "    def isValid(self, value):\n",
    "        return self.condition(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a State class that simply has a name and holds a number of possible transitions (edges to other nodes). The `decide()` simply takes in a `value` argument and checks to see if it meets any of the transition's conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    \"\"\"State docstring.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.transitions = []\n",
    "    def add_transition(self, target, condition):\n",
    "        self.transitions.append(Transition(target, condition))\n",
    "    def decide(self, value):\n",
    "        for t in self.transitions:\n",
    "            if t.isValid(value):\n",
    "                return t.get_state()\n",
    "        return None\n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use what we have made so far in order to create the entire FSM. Simply put, the FSM holds a number of states and keeps track of a current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: unknown\n",
      "current state: lt_op\n",
      "current state: lte_op\n",
      "current state: None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class FSM(object):\n",
    "    \"\"\"FSM docstring.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self.states = {}\n",
    "    def add_state(self, name):\n",
    "        if name not in self.states:\n",
    "            self.states[name] = State(name)\n",
    "    def set_state(self, name):\n",
    "        if name not in self.states:\n",
    "            raise Exception(\"State does not exist: \"+name)\n",
    "        self.state = self.states[name]\n",
    "    def add_rule(self, state, target, condition):\n",
    "        self.add_state(state)\n",
    "        self.add_state(target)\n",
    "        self.states[state].add_transition(self.states[target], condition)\n",
    "    def add_char_rule(self, state, target, char):\n",
    "        self.add_rule(state, target, lambda c: c == char)\n",
    "    def add_regex_rule(self, state, target, regex):\n",
    "        self.add_rule(state, target, lambda c: re.match(regex, c))\n",
    "    def decide(self, value):\n",
    "        self.state = self.state.decide(value)\n",
    "\n",
    "fsm = FSM()\n",
    "fsm.add_char_rule(\"unknown\", \"add_op\", '+')\n",
    "fsm.add_char_rule(\"unknown\", \"lt_op\", '<')\n",
    "fsm.add_char_rule(\"lt_op\", \"lte_op\", '=')\n",
    "fsm.set_state(\"unknown\")\n",
    "print(\"current state:\",fsm.state)\n",
    "fsm.decide('<')\n",
    "print(\"current state:\",fsm.state)\n",
    "fsm.decide('=')\n",
    "print(\"current state:\",fsm.state)\n",
    "fsm.decide('2')\n",
    "print(\"current state:\",fsm.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this is working. Let's create a new FSM class that extends the one above, that way we can initialize it how we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gte_op': '>=', 'string_lit': '^\\\\\"[^\\\\\"]*\\\\\"$', 'begin_paren': '(', 'keyword': '^[a-z]+$', 'neq_op': '!=', 'end_block': '}', 'gt_op': '>', 'end_paren': ')', 'comma_sep': ',', 'id': '^[A-Za-z_][A-Za-z0-9_]*$', 'sub_op': '-', 'add_op': '+', 'end_stmt': ';', 'div_op': '/', 'begin_block': '{', 'assign_op': '=', 'eq_op': '==', 'num_lit': '^(([1-9][0-9]*(\\\\.[0-9]+)?)|(0\\\\.[0-9]+)|0)$', 'lte_op': '<=', 'lt_op': '<', 'mul_op': '*'}\n"
     ]
    }
   ],
   "source": [
    "from res.Wowza.wowza_symbols import kw_table, token_def\n",
    "\n",
    "class LexcialFSM(FSM):\n",
    "    \"\"\"LexicalFSM docstring.\"\"\"\n",
    "    def __init__(self, token_def):\n",
    "        super().__init__()\n",
    "        \n",
    "        #\n",
    "        for t in token_def:\n",
    "            if len(token_def[t]) == 1:\n",
    "                self.add_char_rule(\"unknown\", t, token_def[t])\n",
    "        \n",
    "        #\n",
    "        self.add_char_rule(\"lt_op\", \"lte_op\", '=')\n",
    "        self.add_char_rule(\"gt_op\", \"gte_op\", '=')\n",
    "        self.add_char_rule(\"not_op\", \"neq_op\", '=')\n",
    "        self.add_char_rule(\"assign_op\", \"eq_op\", '=')\n",
    "        \n",
    "        #\n",
    "        self.add_regex_rule(\"lt_op\", \"lte_op\", '=')\n",
    "        \n",
    "        #\n",
    "        self.set_state(\"unknown\")\n",
    "\n",
    "print(token_def)\n",
    "fsm = LexcialFSM(token_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "missing ), unterminated subpattern at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-49991b2c5e8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mscanner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScanner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./res/test.wow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mscanner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-49991b2c5e8a>\u001b[0m in \u001b[0;36mlex\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__peekChar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TOKEN:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/prog-lang/lib/python3.5/re.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \"\"\"Try to apply the pattern at the start of the string, returning\n\u001b[1;32m    162\u001b[0m     a match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfullmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/prog-lang/lib/python3.5/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/prog-lang/lib/python3.5/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/prog-lang/lib/python3.5/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, pattern)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/prog-lang/lib/python3.5/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, nested)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mitemsappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/prog-lang/lib/python3.5/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state)\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m                 raise source.error(\"missing ), unterminated subpattern\",\n\u001b[0;32m--> 781\u001b[0;31m                                    source.tell() - start)\n\u001b[0m\u001b[1;32m    782\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosegroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: missing ), unterminated subpattern at position 0"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from res.Wowza.wowza_symbols import kw_table, token_def\n",
    "\n",
    "class Scanner:\n",
    "    def __init__(self, path):\n",
    "        self.file = open(path, 'r')\n",
    "        self.line = \"\"\n",
    "        self.buffer = \"\"\n",
    "        self.eof = False\n",
    "    def __peekChar(self):\n",
    "        if not self.line:\n",
    "            self.line = self.file.readline()\n",
    "            self.eof = (len(self.line) == 0)\n",
    "        return self.line[0] if not self.eof else \"\"\n",
    "    def __getChar(self):\n",
    "        c = self.__peekChar()\n",
    "        self.line = self.line[1:]\n",
    "        return c\n",
    "    def lex(self):\n",
    "        if self.eof:\n",
    "            return None\n",
    "        \n",
    "        self.buffer = self.__peekChar()\n",
    "        for token in token_def:\n",
    "            if re.match(token_def[token], self.buffer):\n",
    "                print(\"TOKEN:\",token)\n",
    "\n",
    "scanner = Scanner('./res/test.wow')\n",
    "scanner.lex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
